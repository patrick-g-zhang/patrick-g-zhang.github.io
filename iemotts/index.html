<!DOCTYPE html>
<html lang="en">
<head>
  <title>iEmoTTS</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel=¨stylesheet¨ type="text/css" href="pib.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="http://code.jquery.com/jquery-1.11.0.min.js"></script>

  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <script src="./emotion_intensity.js"></script>
    <script src="./disentanglement.js"></script>
    <script src="./cmp_other.js"></script>
    <script src="./emotion_encoder.js"></script>
    <script src="./semi-sup.js"></script>
    <script src="./zero-shot.js"></script>



</head>
<body>

<h1 align="center">
iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis based on Disentanglement between Prosody and Timbre
</h1>


<div class="container">
<h2>Abstract</h2>
The capability of generating speech with specific type of emotion is desired for many applications of human-computer interaction. Cross-speaker emotion transfer is a common approach to generating emotional speech when speech with emotion labels from target speakers is not available for model training. This paper presents a novel cross-speaker emotion transfer system, named iEmoTTS. The system is composed of an emotion encoder, a prosody predictor, and a timbre encoder. The emotion encoder extracts the identity of emotion type as well as the respective emotion intensity from the mel-spectrogram of input speech. The emotion intensity is measured by the posterior probability that the input utterance carries that emotion. The prosody predictor is used to provide prosodic features for emotion transfer. The timber encoder provides timbre-related information for the system. Unlike many other studies which focus on disentangling speaker and style factors of speech, the iEmoTTS is designed to achieve cross-speaker emotion transfer via disentanglement between prosody and timbre. Prosody is considered as the main carrier of emotion-related speech characteristics and timbre accounts for the essential characteristics for speaker identification. Zero-shot emotion transfer, meaning that speech of target speakers are not seen in model training, is also realized with iEmoTTS. Extensive experiments of subjective evaluation have been carried out. The results demonstrate the effectiveness of iEmoTTS as compared with other recently proposed systems of cross-speaker emotion transfer. It is shown that iEmoTTS can produce speech with designated emotion type and controllable emotion intensity. With appropriate information bottleneck capacity, iEmoTTS is able to effectively transfer emotion information to a new speaker. Audio samples are publicly available\footnote{https://patrick-g-zhang.github.io/iemotts/}.
</div>

<div class="container">
    <h2>Audio Samples</h2>
          1. Some real recordings are presented in this part. </br>
          2. The utterance of "sad" emotion is from corpus <b>Multi-S60-E3</b>, while the utterances of other emotions are from corpus <b>Child-S1-E6</b>.</br>
          3. One target speaker from corpus <b>VA-S2</b>. </br>
          4. One target speaker from corpus <b>Read-S40</b>. </br>

      <h3>Audio Samples of Different Emotion Types</h2>
        <table class="table table-hover" > 
          <thead>
            <tr>
              <th >Amazement</th>
              <th >Anger </th>
              <th>Disgust</th>
            </tr>
          </thead>
          <tbody>

            <tr>
              <td> <audio src="./samples/emotion/TGEmotionalchild_amazed_240001.wav" controls=""></audio> </td>
              <td> <audio src="./samples/emotion/TGEmotionalchild_angry_220001.wav" controls=""></audio> </td>
              <td> <audio src="./samples/emotion/TGEmotionalchild_disgusted_260002.wav" controls=""></audio> </td>
           </tr>  

          </tbody>

        </table>

      <table class="table table-hover" > 
          <thead>
            <tr>
              <th>Fear</th>
              <th>Happiness</th>
              <th>Poorness</th>
              <th>Sadness</th>
            </tr>
          </thead>
          <tbody>

            <tr>
              <td> <audio src="./samples/emotion/TGEmotionalchild_fear_210002.wav" controls=""></audio> </td>
              <td> <audio src="./samples/emotion/TGEmotionalchild_happy_250002.wav" controls=""></audio> </td>
              <td> <audio src="./samples/emotion/TGEmotionalchild_poor_230002.wav" controls=""></audio> </td>
              <td> <audio src="./samples/emotion/TGE0101M_sad_001.wav" controls=""></audio> </td>
           </tr>  

          </tbody>

        </table>

        <h3>Audio Samples of one Target Speakers from <b>VA-S2</b></h2>
                    <table class="table table-hover" > 

            <tr>
              <td> <audio src="./samples/TGLF/TGLF_ch_001035.wav" controls=""></audio> </td>
              <td> <audio src="./samples/TGLF/TGLF_ch_002063.wav" controls=""></audio> </td>
              <td> <audio src="./samples/TGLF/TGLF_ch_002666.wav" controls=""></audio> </td>
           </tr>  

          </tbody>

        </table>


        <h3>Audio Samples of one Target Speaker from <b>Read-S40</b></h2>
           <table class="table table-hover" > 

            <tr>
              <td> <audio src="./samples/TGE0146F/TGE0146F_neutral_025.wav" controls=""></audio> </td>
              <td> <audio src="./samples/TGE0146F/TGE0146F_neutral_027.wav" controls=""></audio> </td>
              <td> <audio src="./samples/TGE0146F/TGE0146F_neutral_034.wav" controls=""></audio> </td>
           </tr>  

          </tbody>

        </table>





    <h2>Performance Evaluation</h2>
    <i>
    These examples are sampled from the subjective evaluation set for <b> Table II </b> in the paper.
    </i>

  <table class="table table-hover" id="cmp_other_table" > 
    <thead>
      <tr>
        <th>Emotion</th>
        <th>iEmoTTS</th>
        <th>Trans-CLN</th>
        <th>Trans-Pros</th>
      </tr>
    </thead>
    <tbody>
      
      <!-- first group -->
      <tr id="cmp_other_table_row1">
        <td class="tg-dvpl" rowspan="2" >Amazement</td>
        <td colspan="3"> <span></span> </td>
      </tr>

      <tr id="cmp_other_table_row2">
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
     </tr>     
    </tbody>

  </table>


    <h2>Effectiveness of Disentanglement</h2>
    <i>
      These examples are sampled from the subjective evaluation set for <b> Table III </b> in the paper.
    </i>

  <table class="table table-hover" id="disentanglement_table" > 
    <thead>
      <tr>
        <th>Emotion</th>
        <!-- <th >Target Speaker</th> -->
        <th>iEmoTTS</th>
        <th>iEmoTTS-NP</th>
        <th>iEmoTTS-SE</th>
        <th>iEmoTTS-SENP</th>
      </tr>
    </thead>
    <tbody>
      
      <!-- first group -->
      <tr id="disentanglement_table_row1">
        <td class="tg-dvpl" rowspan="2" >Amazement</td>
        <!-- <td class="tg-dvpl" rowspan="2"> Male </td> -->
        <td colspan="4"> <span></span> </td>
      </tr>

      <tr id="disentanglement_table_row2">
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
     </tr>

<!--       <tr id="disentanglement_table_row3">
        <td class="tg-dvpl" rowspan="2"> Female </td>
        <td colspan="3"> <span></span> </td>
      </tr>

      <tr id="disentanglement_table_row4">
          <td> <audio src="" controls=""></audio> </td>
          <td> <audio src="" controls=""></audio> </td>
          <td> <audio src="" controls=""></audio> </td>
      </tr>   -->    
    </tbody>

  </table>

    <!-- emotion intensity -->

    <h2>Emotion Intensity Control</h2>
    <i>
    These examples are sampled from the subjective evaluation set for <b> Table IV </b> in the paper.
    <i>
  <h3>Source Speaker</h3>

  <table class="table table-hover", id="source_emotion_intensity">
    <thead>
      <tr>
        <th>Emotion</th>
        <th>System</th>
        <th >Low</th>
        <th>Moderate</th>
        <th>High</th>
      </tr>
    </thead>
    <tbody>
            <!-- first group -->
      <tr id="source_emotion_intensity_text">
        <td class="tg-dvpl" rowspan="3" > </td>
        
        <td colspan="6", class="text-center">
            <span>
            </span>
        </td>
      </tr>

      <tr id="source_emotion_intensity_url">
        <td class="tg-dvpl" rowspan="1"> </td>
        <td>
                <audio src="dsadsa" controls=""></audio>
        </td>


        <td>
                <audio src="" controls=""></audio>
        </td>  
        

        <td>
                <audio src="" controls=""></audio>
        </td>  
      </tr>
      
    </tbody>
  </table>

 <h3>Target Speaker</h3>

<table class="table table-hover", id="target_emotion_intensity">
    <thead>
      <tr>
        <th>Emotion</th>
        <th>System</th>
        <th >Low</th>
        <th>Moderate</th>
        <th>High</th>
      </tr>
    </thead>
    <tbody>
      
    </tbody>
  </table>



    <h2>Zero-shot Cross-speaker Emotion Transfer</h2>
    <i>
      These examples are sampled from the subjective evaluation set for <b> Table V </b> in the paper.
    </i>
    
  <table class="table table-hover" id="zero_shot_table" > 
    <thead>
      <tr> <th>Emotion</th> <th>IB-full</th> <th>IB-large</th> <th>IB-default</th> <th>IB-small</th> </tr>
    </thead>
    <tbody>
      <tr id="zero_shot_table_row1">
        <td class="tg-dvpl" rowspan="2" > </td>
        <td colspan="6"> <span></span> </td>
      </tr>
      <tr id="zero_shot_table_row2">
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
     </tr>  
      
    </tbody>

  </table>



    <h2>Effectiveness of Emotion Encoder</h2>
    <i>
      These examples are sampled from the subjective evaluation set for <b> Table VI </b> in the paper.
    </i>

  <table class="table table-hover" id="emotion_enc_table" > 
    <thead>
      <tr> <th>Emotion</th> <th>iEmoTTS</th> <th>iEMOTTS-SER</th> <th>iEMOTTS-WoEI</th> </tr>
    </thead>
    <tbody>
      <tr id="emotion_enc_table_row1">
        <td class="tg-dvpl" rowspan="2" > </td>
        <td colspan="5"> <span></span> </td>
      </tr>
      <tr id="emotion_enc_table_row2">
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
     </tr>  
      
    </tbody>

  </table>


     <h2>Effectiveness of Semi-Supervised Strategy</h2>
     <i>
      These examples are sampled from the subjective evaluation set for <b> Table VII </b> in the paper.
    </i>
    

  <table class="table table-hover" id="semi_sup_table" > 
    <thead>
      <tr> <th>Emotion </th> <th>iEmoTTS</th> <th>iEMOTTS-NEU</th> <th>iEMOTTS-NEU2</th> </tr>
    </thead>
    <tbody>
      <tr id="semi_sup_table_row1">
        <td class="tg-dvpl" rowspan="2" > </td>
        <td colspan="5"> <span></span> </td>
      </tr>
      <tr id="semi_sup_table_row2">
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
        <td> <audio src="" controls=""></audio> </td>
     </tr>  
      
    </tbody>

  </table>



</div>


</body>
</html>

